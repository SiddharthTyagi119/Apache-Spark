1--Return df column names and data types
df.dtypes

2--Display the content of df
df.show()

3--Return first n rows
df.head(2)

4--Return first row
df.first()
 
5--Return the first n rows
df.take(2)
 
6--Return the schema of df
df.schema 

7--Drop duplicates
df.dropDuplicates()

8--Update column name
df = df.withColumnRenamed('telePhoneNumber', 'phoneNumber')

9--Drop column
df = df.drop("address", "phoneNumber")
df = df.drop(df.address).drop(df.phoneNumber)

10--Between
df.select(df.age.between(22, 24)) .show() 

11--Startswith - Endswith
 df.select(df.lastame.endswith("na")).show()
 df.select(df.firstname.startswith("sa")).show()
 
12--Substring
df.select(df.firstname.substr(1, 2).alias("name")).collect()

13--Like
df.select("firstName",df.lastName.like("Smith")),show()

14--When
from pyspark.sql import functions as F
df.select("firstName",F.when(df.age > 19, 1).otherwise(0)).show()

df[df.firstName.isin("Jane","Boris")].collect() 

15--Select
df.select("name","age").show()
df.select(df['age'] > 24).show()

16--Compute summary statistics
df.describe().show()

17--Returns the columns of df
df.columns

18--Count the number of rows in df
df.count()

19--Count the number of distinct rows
df.distinct().count()

20--Print Schema
df.srintSchema()

21--Print the logical and physical plans
df.explain()

22-Group by age, count the members in the group
df.groupBy("age").count().show() 

23-Filter
df.filter(df["age"]>24).show()

24--Sort
df.sort(peopledf.age.desc()).collect()
df.sort("age", ascending=False).collect()
df.orderBy(["age","city"],ascending=[0,1]).collect()

25--Missing & Replacing Values
Replace null values
df.na.fill(50).show()

Return new df omitting rows with null values
df.na.drop().show() 

Return new df replacing one value with other
df.na.replace(10, 20).show()

26--Repartitioning
df with 10 partitions
df.repartition(10).rdd.getNumPartitions()

df with 1 partition
df.coalesce(1).rdd.getNumPartitions()

27-View
df.createTempView("customer")

28-Query View
df5 = spark.sql("SELECT * FROM customer").show()

29-Write & Save to Files
df.select("firstName", "city").write..save("namesAndAges.json",format="json")

30-Stop Spark Session
spark.stop()

31-Covariance
df.stat.cov('name','age')

32-Correlation
df.stat.corr('name','age')

33-Crosstab
df..stat.crosstab('loan','grade').show()

34-Frequency Item
freq=df.stat.freqItems(['purpose','grade'],0.3)
freq.collect()

35-Check NULL values
from pyspark.sql.functions import isnan,when,count,col
df.select([count(when(isnan(c) | col(c).isNull(),c)).alias(c) for c in df_sel.columns]).show()

36-Drop NULL values
df=df.na.drop("all",subset=["loan_status"])

37-To replace the null values with avg of values
def fill_avg(df,column_name):
         return df.select(colname).agg(avg(colname))
 rev_avg=fill_avg(df,'colname')
 





