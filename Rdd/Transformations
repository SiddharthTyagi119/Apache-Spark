1--Convert python list in Rdd
data= [1,2,3,4,5] 
rDD = sc.parallelize(data,3)
rDD.collect()

2--Create a rdd from a list and convert back it in list
A=sc.parallelize([2,4,7])
L=A.collect()
print(type(L))
print(L)

3--Lambda function to multiply three numbers-
rdd = sc.parallelize([1,2,3,4])
rdd.reduce(lambda x,y: x * y)

4--Convert unorder list into rdd
rDD=sc.parallelize([5,3,1,2])
rDD.takeOrdered(4, lambda s: 1*s)

5--Show the number of elements
B= sc.parallelize([2,4,7])
B.count()

6--To save the output-
w= sc.parallelize([2,4,7])
w.count()
w.saveAsTextFile("/File Store /rDDData")

7--To see the output
%sh
ls -ltr /dbfs/File Store /rDDData
cat /dbfs/File Store /rDDData/*

8--Find the shortest word in the list of words-
words=['this','is','the','best','mac','ever']
wordRDD= sc.parallelize(words)
wordRDD.reduce(lambda w,v: w if len(w) < len(v) else v)

9--Lambda fun. To subtract numbers-
B=sc.parallelize([1,3,5,2])
B.reduce(lambda x,y: x-y )


10--Check by default prtition
Z=sc.parallelize(range(1000000))
print(Z.getNumPartitions())

11--Change the number of default partition
A=sc.parallelize(range(10))
print(A.getNumPartitions())
D=A.repartition(6)
print(D.getNumPartitions())

                OR
Add number of desired partition number in paranthesis
A=sc.parallelize(range(10),6)
print(A.getNumPartitions())

12--Map transformation
rdd2=sc.parallelize([1,2,3])
rdd2.map(lambda x:x*2).collect()

13--Filter function
rdd=sc.parallelize([1,2,3,4,5,6,7])
rdd.filter(lambda x:x%2==0).collect()

df.filter('confirmed>100').sort(col('confirmed')).show()

14--Distinct Function
rdd=sc.parallelize([1,2,3,4,6])
rdd.distinct().collect()

15--Union
rdd1=sc.parallelize([1,1,2,3])
rdd2=sc.parallelize(['a','b',1])
print('rdd1=',rdd1.collect())
print('rdd1=',rdd2.collect())
print('union as bag =',rdd1.union(rdd2).collect())
print('union as bag =',rdd1.union(rdd2).distinct().collect())


16--Map and Flat Map
text=["you are my sunshine","my only sunshine"]
text=sc.parallelize(text)
         
## create one list for one sentence
print('map:',text.map(lambda line: line.split(" ")).collect())

## create a single list of words by combining the words from all of the lines
print('flatmap:',text.flatMap(lambda line: line.split(" ")).collect())

17--Reduce By Key
It will add the element of same key
rdd =sc.parallelize([(2,2),(1,4),(2,6)])
print("Original RDD :", rdd.collect())
print("after transformation: ", rdd.reduceByKey(lambda a,b:a+b).collect())


18--Sort By Key
It will sort the values on the basis of key 
rdd =sc.parallelize([(2,2),(1,4),(3,6),(4,8)])
print("Original RDD :", rdd.collect())
print("after transformation: ", rdd.sortByKey().collect())


19--Map value
Its double the value
rdd =sc.parallelize([(1,2),(2,4),(2,6)])
print("Original RDD :", rdd.collect())
print("after transformation: ", rdd.mapValues(lambda x: x*x).collect())

20--Combine narrow and wide transformation
rdd =sc.parallelize([(1,2),(2,4),(2,6)])
print("Original RDD :", rdd.collect())
print("After transformation: ",rdd.groupByKey().mapValues(lambda x:[a+a for a in x]).collect())
print("After transformation: ",rdd.groupByKey().mapValues(sum).collect())


21--Create an empty RDD with partition
rdd2 = spark.sparkContext.parallelize([],10)#This creates 10 partitions
rdd2.collect()

23--Return the union of this RDD and another one
rdd = sc.parallelize([1, 1, 2, 3])
(rdd + rdd).collect()

24--Return the intersection of this RDD and another one
rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
rdd1.intersection(rdd2).collect()

25--Return the Cartesian product of this RDD and another one
rdd = sc.parallelize([1, 2])
sorted(rdd.cartesian(rdd).collect())


26--Save this RDD as a text file, using string representations of elements.
rdd7.saveAsTextFile("rdd7.txt")

27--Return each value in self that is not contained in other
x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 3)])
y = sc.parallelize([("a", 3), ("c", None)])
sorted(x.subtract(y).collect())

27--Sorts this RDD by the given keyfunc
tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()

28--Select
df.("column name").show()

29--Sort
df.filter("Country=USA").sort(col("Date"),ascending=False).show()

30--OrderBy
df.filter("Country=USA").orderBy(col("Date"),ascending=False).show()

31--SortByPartition
df.filter("Country=USA").([col("Date"),col("confimed")],ascending=False).show()

32--Aggregate
df.agg({"Date":"max"}).collect()

33-Pivot
df.groupby("country").pivot("date").agg(F.sum("confirmed")).show()






       




