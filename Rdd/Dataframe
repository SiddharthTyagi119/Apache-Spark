1--Create a row type Rdd
from pyspark.sql import SQLContext
from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType
some_rdd= sc.parallelize([Row(name=u'john',age=19),
                          Row(name=u'sam',age=20),
                          Row(name=u'brock',age=32)])
some_rdd.collect()

2--Convert an RDD into Data Frame
#Here some_rdd is a RDD
some_df=spark.createDataFrame(some_rdd)
some_df.printSchema()
some_df.show()

3--To print the type
print(type(some_rdd),type(some_df))   

4--Create a dataframe without schema and provide the column names explicitly
data=[("java","20000"),("python","10000"),("scala","3000")]
rdd=sc.parallelize(data)
df1=spark.createDataFrame(rdd)
df1.printSchema()
df1.show()

##Now add columns name
 df2=rdd.toDF(["language","usercounts"])
 df2.show()
 
 4-- To print the schema of a dataframe
 <Data frame name>.printSchema()
 
 5--Create a rdd with single value list
 data2=[("java",),("python",),("scala",)]
rdd2=sc.parallelize(data2)
df2=spark.createDataFrame(rdd2)
df2.printSchema()
df2.show()

6--Create the dataframe from the rdd of tuples (rather than rows) and provide the schema explicitly
another_rdd=sc.parallelize([("java",20),("python",10),("scala",30)])
#Schema with two fields- person_name and person_age
schema=StructType([StructField("Person_name",StringType(),False),
                  StructField("Person_age",IntegerType(),False),])
                  
##Create a dataframe by applying the schema to rdd and print the schema
another_df=sqlContext.createDataFrame(another_rdd,schema)
another_df.printSchema()

7--Create a dataframe from a json file
path="//"
people_df=spark.read.json(path)
print("people is a",type(people))
people.printSchema()
people.show()
people.printSchema()

8--To save the data of a dataframe
df.write.parquet("/FileStore/tables/output5")

9--Creating Nested Dataframe
import pyspark.sql import *

#Create example data- Department and employee
#Create the department df
department1=Row(id='123456',name='Computer Science')
department2=Row(id='123457',name='Data Science')
department3=Row(id='123458',name='Data Analytics')
department4=Row(id='123459',name='BlockChain')

#Create employee df
Employee=Row("FirstName","LastName","email","salary")
employee1=Employee('sid','tyagi','sidtyagi05@gmail.com',20000)
employee2=Employee('sam','sharma','sidtyagi06@gmail.com',70000)
employee3=Employee('abhinav','baliyan','sidtyagi07@gmail.com',90000)
employee4=Employee('chinmay','sharma','sidtyagi08@gmail.com',40000)
employee5=Employee('john','cena','sidtyagi09@gmail.com',30000)

#Create the department with employee instances from departments and employee
departmentwithemployee1= Row(department=department1,employees=[employee1,employee2])
departmentwithemployee2= Row(department=department2,employees=[employee2,employee1])
departmentwithemployee3= Row(department=department3,employees=[employee4,employee3])
departmentwithemployee4= Row(department=department4,employees=[employee3,employee4])

departmentwithEmployeeSeq1=[departmentwithemployee1 + departmentwithemployee2]
df1=spark.createDataFrame(departmentwithEmployeeSeq1)
display (df1)
departmentwithEmployeeSeq2=[departmentwithemployee3 + departmentwithemployee4]
df2=spark.createDataFrame(departmentwithEmployeeSeq2)
display (df2)



















 
 

