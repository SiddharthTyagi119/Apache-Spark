1--Create a row type Rdd
from pyspark.sql import SQLContext
from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType
some_rdd= sc.parallelize([Row(name=u'john',age=19),
                          Row(name=u'sam',age=20),
                          Row(name=u'brock',age=32)])
some_rdd.collect()

2--Convert an RDD into Data Frame
#Here some_rdd is a RDD
some_df=spark.createDataFrame(some_rdd)
some_df.printSchema()
some_df.show()

3--To print the type
print(type(some_rdd),type(some_df))   

4--Create a dataframe without schema and provide the column names explicitly
data=[("java","20000"),("python","10000"),("scala","3000")]
rdd=sc.parallelize(data)
df1=spark.createDataFrame(rdd)
df1.printSchema()
df1.show()

##Now add columns name
 df2=rdd.toDF(["language","usercounts"])
 df2.show()
 
 4-- To print the schema of a dataframe
 <Data frame name>.printSchema()
 
 5--Create a rdd with single value list
 data2=[("java",),("python",),("scala",)]
rdd2=sc.parallelize(data2)
df2=spark.createDataFrame(rdd2)
df2.printSchema()
df2.show()

6--Create the dataframe from the rdd of tuples (rather than rows) and provide the schema explicitly
another_rdd=sc.parallelize([("java",20),("python",10),("scala",30)])
#Schema with two fields- person_name and person_age
schema=StructType([StructField("Person_name",StringType(),False),
                  StructField("Person_age",IntegerType(),False),])
                  
##Create a dataframe by applying the schema to rdd and print the schema
another_df=sqlContext.createDataFrame(another_rdd,schema)
another_df.printSchema()

7--Create a dataframe from a json file
path="//"
people_df=spark.read.json(path)
print("people is a",type(people))
people.printSchema()
people.show()
people.printSchema()

8--To save the data of a dataframe
df.write.parquet("/FileStore/tables/output5")

9--Creating Nested Dataframe
from pyspark.sql import *

#Create example data- Department and employee
#Create the department df
department1=Row(id='123456',name='Computer Science')
department2=Row(id='123457',name='Data Science')
department3=Row(id='123458',name='Data Analytics')
department4=Row(id='123459',name='BlockChain')

#Create employee df
Employee=Row("FirstName","LastName","email","salary")
employee1=Employee('sid','tyagi','sidtyagi05@gmail.com',20000)
employee2=Employee('sam','sharma','sidtyagi06@gmail.com',70000)
employee3=Employee('abhinav','baliyan','sidtyagi07@gmail.com',90000)
employee4=Employee('chinmay','sharma','sidtyagi08@gmail.com',40000)
employee5=Employee('john','cena','sidtyagi09@gmail.com',30000)

#Create the department with employee instances from departments and employee
departmentwithemployee1= Row(department=department1,employees=[employee1,employee2])
departmentwithemployee2= Row(department=department2,employees=[employee2,employee1])
departmentwithemployee3= Row(department=department3,employees=[employee4,employee3])
departmentwithemployee4= Row(department=department4,employees=[employee3,employee4])

departmentwithEmployeeSeq1=[departmentwithemployee1 + departmentwithemployee2]
df1=spark.createDataFrame(departmentwithEmployeeSeq1)
display (df1)
departmentwithEmployeeSeq2=[departmentwithemployee3 + departmentwithemployee4]
df2=spark.createDataFrame(departmentwithEmployeeSeq2)
display (df2)

10--Join two dataframes
uniondf=df1.union(df2)
display(uniondf)

11-Creates Empty RDD
emptyRDD = spark.sparkContext.emptyRDD()
print(emptyRDD)
 
                 OR
rdd2= spark.sparkContext.parallelize([])
print(rdd2)


12-Create empty DataFrame from empty RDD
##SCHEMA
from pyspark.sql.types import StructType,StructField, StringType
schema = StructType([
  StructField('firstname', StringType(), True),
  StructField('middlename', StringType(), True),
  StructField('lastname', StringType(), True)
  ])
  #Empty RDD
emptyRDD = spark.sparkContext.emptyRDD()
#Create empty DataFrame from empty RDD
df = spark.createDataFrame(emptyRDD,schema)
df.printSchema()

13-Create empty DataFrame from empty RDD
df2 = spark.createDataFrame([], schema)
df2.printSchema()

14--Create empty DatFrame with no schema (no columns)
df33 = spark.createDataFrame([], StructType([]))
df33.printSchema()

15--Create DataFrame with schema
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])
 
df = spark.createDataFrame(data=data2,schema=schema)
df.printSchema()
df.show(truncate=False)

16--Creating DataFrame from CSV
df2 = spark.read.csv("/src/resources/file.csv")

17--Creating from text (TXT) file
df2 = spark.read.text("/src/resources/file.txt")

18--Convert PySpark Dataframe to Pandas DataFrame
#Create a dataframe
import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()

data = [("James","","Smith","36636","M",60000),
        ("Michael","Rose","","40288","M",70000),
        ("Robert","","Williams","42114","",400000),
        ("Maria","Anne","Jones","39192","F",500000),
        ("Jen","Mary","Brown","","F",0)]

columns = ["first_name","middle_name","last_name","dob","gender","salary"]
pysparkDF = spark.createDataFrame(data = data, schema = columns)
pysparkDF.printSchema()
pysparkDF.show(truncate=False)

#Convert PySpark Dataframe to Pandas DataFrame
pandasDF = pysparkDF.toPandas()
print(pandasDF)

19--Checking if a Column Exists in a DataFrame
print(df.schema.fieldNames.contains("firstname"))
print(df.schema.contains(StructField("firstname",StringType,true)))

20--Select Single & Multiple Columns 
df.select(df["firstname"],df["lastname"]).show()
df.select("firstname","lastname").show()

21--Select All Columns
df.select([col for col in df.columns]).show()
df.select("*").show()

22--Select Columns by Index
df.select(df.columns[2:4]).show(3)

23--Select Nested Struct Columns 
df2.select("name").show(truncate=False)

24--Get Distinct Rows
distinctDF = df.distinct()
print("Distinct count: "+str(distinctDF.count()))
distinctDF.show(truncate=False)


25--Distinct of Selected Multiple Columns
dropDisDF = df.dropDuplicates(["department","salary"])
print("Distinct count of department & salary : "+str(dropDisDF.count()))
dropDisDF.show(truncate=False)


26--DataFrame sorting using the sort() function
df.sort("department","state").show(truncate=False)

                         OR
df.orderBy("department","state").show(truncate=False)

27--Sort by Asc
df.sort(col("department").asc(),col("state").asc()).show(truncate=False)

28--Sort by Desc
df.sort(col("department").desc(),col("state").desc()).show(truncate=False)

29--DataFrame filter() with Column Condition
=Equal
df.filter(df.state == "NY").show(truncate=False)
df.filter( (df.state  == "OH") & (df.gender  == "M") ) \
    .show(truncate=False)  

!=Not Equal
df.filter(df.state != "OH") \
    .show(truncate=False) 
    
30--Filter Based on Starts With, Ends With, Contains
#startswith
df.filter(df.state.startswith("N")).show()

#using endswith
df.filter(df.state.endswith("H")).show()

#contains
df.filter(df.state.contains("H")).show()

31--Filtering on Nested Struct columns
//Struct condition
df.filter(df.name.lastname == "Williams") \
    .show(truncate=False) 
    
32--Retrive all the data as an Array of Row type
dataCollect = deptDF.collect()
print(dataCollect)

33-python for loop to retrive array type data
for row in dataCollect:
    print(row['dept_name'] + "," +str(row['dept_id']))
    
34--Returns the value of the first row & first column
deptDF.collect()[0][0]

35--To just return certain elements of a DataFrame
dataCollect2= deptDF.select("dept_name").collect()
print(dataCollect2)

36--Change DataType
df.withColumn("salary",col("salary").cast("Integer")).show()

37--Create a Column from an Existing
df.withColumn("CopiedColumn",col("salary")* -1).show()

38--Rename Column Name
df.withColumnRenamed("gender","sex") \
  .show(truncate=False) 
  
39--Drop Column 
df.drop("salary") \
  .show() 
  
40--GroupBy for min,max,count,avg,mean
df.groupBy("department").sum("salary").show(truncate=False)

41--Group by on multiple columns
df.groupBy("department","state") \
    .sum("salary","bonus") \
    .show(false)
    
42--Running more aggregates at a time

df.groupBy("department") \
    .agg(sum("salary").alias("sum_salary"), \
         avg("salary").alias("avg_salary"), \
         sum("bonus").alias("sum_bonus"), \
         max("bonus").alias("max_bonus") \
     ) \
    .show(truncate=False)

43--Using filter on aggregate data
df.groupBy("department") \
    .agg(sum("salary").alias("sum_salary"), \
      avg("salary").alias("avg_salary"), \
      sum("bonus").alias("sum_bonus"), \
      max("bonus").alias("max_bonus")) \
    .where(col("sum_bonus") >= 50000) \
    .show(truncate=False)
    
44--Count
df.count()

45-Info Schema
df.printSchema()

46-To create a tempory table
temp_table="loanstats"
df.createOrReplaceTempView(temp_table)

    

